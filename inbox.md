# inbox

## code less papers

* [Improving Sentence Representations with Multi-view Frameworks](https://arxiv.org/abs/1810.01064)
* [Efficient Contextual Representation Learning Without Softmax Layer](https://arxiv.org/abs/1902.11269)
* [Cloze-driven Pretraining of Self-attention Networks](https://arxiv.org/abs/1903.07785)
* [Unified Language Model Pre-training forNatural Language Understanding and Generation](https://arxiv.org/abs/1905.03197)
* [HIBERT: Document Level Pre-training of Hierarchical BidirectionalTransformers for Document Summarization](https://arxiv.org/abs/1905.06566)
* [Probing Multilingual Sentence Representations With X-PROBE](https://arxiv.org/abs/1906.05061): [xprobe](https://github.com/ltgoslo/xprobe)
* [GLOSS: Generative Latent Optimization of Sentence Representations](https://arxiv.org/abs/1907.06385)
* [SpanBERT: Improving Pre-training byRepresenting and Predicting Spans](https://arxiv.org/abs/1907.10529)
* [An Unsupervised Character-Aware Neural Approach to Word and Context Representation Learning](https://arxiv.org/abs/1908.01819)
* [ViCo: Word Embeddings from Visual Co-occurrences](https://arxiv.org/abs/1908.08527)
* [Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks](https://arxiv.org/abs/1909.00964)
* [MULE: Multimodal Universal Language Embedding](https://arxiv.org/abs/1909.03493)

## not free
* [Unsupervised word embeddings capture latent knowledge from materials science literature](https://www.nature.com/articles/s41586-019-1335-8): [mat2vec](https://github.com/materialsintelligence/mat2vec)
